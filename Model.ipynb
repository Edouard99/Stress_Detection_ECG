{"cells":[{"cell_type":"markdown","metadata":{"id":"USaorKo7DP9B"},"source":["# Base"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31244,"status":"ok","timestamp":1662666285989,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"gr2Cj9rxQ_NT","outputId":"090b8a4e-5f7d-4062-d4f8-d12597775907"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","import matplotlib.pyplot as plt \n","import numpy as np\n","import random\n","import json\n","import seaborn as sns\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DIR_DATA=\"/content/data/Dataset/\" #Path where your dataset files are stored, this folder contains WESADECG_S2.json,WESADECG_S3.json,...WESADECG_Sk.json files\n","DIR_NET_SAVING=\"/content/data/net/\" #Path where model's weights are saved for each training\n","DIR_RESULTS=\"/content/data/\" #Path to save Results of the network (loss function)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1662666299608,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"9Qy3Go4cE7EZ","outputId":"24581a7b-964c-477a-b32f-aa14cf369112"},"outputs":[],"source":["manualSeed=1\n","torch.manual_seed(manualSeed)\n","random.seed(manualSeed)\n","np.random.seed(manualSeed)\n","g = torch.Generator()\n","g.manual_seed(manualSeed)"]},{"cell_type":"markdown","metadata":{"id":"o4lE6XdnkRCt"},"source":["# Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1662666299609,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"emSxBk7ikS5m"},"outputs":[],"source":["def suppr(dic):\n","  \"\"\" Delete extremums (irrelevant extract due to weird peak detections) from a dictionnary (a subject)  \n","  \"\"\"\n","  bornemax=np.quantile(dic[\"features\"],0.99,axis=0)\n","  bornemin=np.quantile(dic[\"features\"],0.01,axis=0)\n","  indicesmauvais=np.where(np.sum(np.add(bornemin>np.array(dic[\"features\"]),np.array(dic[\"features\"])>bornemax),axis=1)>0)[0]\n","  k=0\n","\n","  for i in indicesmauvais:\n","    del dic[\"features\"][i-k]\n","    del dic[\"label\"][i-k]\n","    k+=1\n","\n","  return dic\n","\n","def extract_ds_from_dict(data):\n","  \"\"\" Delete the irrelevant extract for each state and return the dataset as a dictionnary\"\"\"\n","  Letat=[]\n","\n","  for i in range(0,4):\n","    dictio={}\n","    features=[data[\"features\"][j] for j in np.where(np.array(data[\"label\"])==i+1)[0]] \n","    label=[data[\"label\"][j] for j in np.where(np.array(data[\"label\"])==i+1)[0]]\n","    dictio[\"features\"]=features\n","    dictio[\"label\"]=label\n","    Letat.append(dictio.copy())\n","\n","  neutr=Letat[0]\n","  stress=Letat[1]\n","  amu=Letat[2]\n","  med=Letat[3]\n","  neutr=suppr(neutr)\n","  stress=suppr(stress)\n","  amu=suppr(amu)\n","  med=suppr(med)\n","  features=[]\n","  label=[]\n","  dict_id={}\n","\n","  for m in range(0,4):\n","    dictio=Letat[m]\n","    features+=[x for x in dictio[\"features\"]] \n","    label+=[x for x in dictio[\"label\"]]\n","\n","  dict_id[\"features\"]=features\n","  dict_id[\"label\"]=label\n","  return dict_id.copy()\n","\n","def conf_mat(net,datal,trsh):\n","  \"\"\" Compute 2 confusion matrixes according to a dataloader and a threshold for state prediction (above the threshold the subject is \n","  under stress condition) \n","  \n","  2x2 confusion matrix : Stress and No stress as label for line and column\n","  3x4 confusion matrix : {Total ; Stress ; No stress} as label for the lines\n","                         {Neutral ; Stress ; Amusement; Meditation} as label for the columns\n","  \"\"\"\n","  x=datal[0].float().to(device)\n","  y=net(x).view(-1)\n","  pred=(y>trsh).int()\n","  label=datal[1].float().to(device).view(-1).int()\n","  num=datal[2].float().to(device).int()\n","  comp=torch.eq(label,pred).int()\n","  mat_label=np.zeros((2,4))\n","  mat_nolbl=np.zeros((2,2))\n","\n","  for i in range(0,4):\n","    tens=torch.where(num==i+1,1,0)\n","    numtot=torch.sum(tens).item()\n","    num_G=torch.sum(torch.where(torch.mul(tens,comp)==1,1,0)).item()\n","\n","    if i ==1:\n","      mat_nolbl[0,0]+=num_G\n","      mat_nolbl[1,0]+=numtot-num_G\n","      mat_label[0,i]=num_G\n","      mat_label[1,i]=numtot-num_G\n","\n","    else:\n","      mat_nolbl[1,1]+=num_G\n","      mat_nolbl[0,1]+=numtot-num_G\n","      mat_label[1,i]=num_G\n","      mat_label[0,i]=numtot-num_G\n","\n","\n","  return mat_label,mat_nolbl\n","\n","def fusion_dic(list_dic):\n","  \"\"\" Merge dictionnaries (dataset) from a list (each dictionnary represent the dataset of a subject \"\"\"\n","  features=[]\n","  label=[]\n","  dic_f={}\n","\n","  for dic in list_dic:\n","    features+=dic[\"features\"]\n","    label+=dic[\"label\"]\n","\n","  dic_f[\"features\"]=features\n","  dic_f[\"label\"]=label\n","  return dic_f\n","\n","def proportion(dic, indice, prop):\n","  \"\"\" Return a balanced dataset (reduced by the factor prop) for a balanced training\"\"\"\n","  tot=len(indice)\n","  features=[dic[\"features\"][j] for j in indice[::int(np.ceil(tot/prop))]]\n","  label=[dic[\"label\"][j] for j in indice[::int(np.ceil(tot/prop))]]\n","  return features,label\n","\n","def eq_dic(dic):\n","  \"\"\" Return a balanced dataset from the dictionnary of a subject (same ammount of data for neutral/stress/amusement/relax condition)\"\"\"\n","  indice_neutr=np.where(np.array(dic[\"label\"])==1)[0]\n","  indice_stress=np.where(np.array(dic[\"label\"])==2)[0]\n","  indice_amu=np.where(np.array(dic[\"label\"])==3)[0]\n","  indice_med=np.where(np.array(dic[\"label\"])==4)[0]\n","  nbr_neutr=len(indice_neutr)\n","  nbr_stress=len(indice_stress)\n","  nbr_amu=len(indice_amu)\n","  nbr_med=len(indice_med)\n","  prop=min([3*nbr_neutr,nbr_stress,3*nbr_amu,3*nbr_med])\n","  prop_stress=prop\n","  prop_neutr=int(0.333*prop)\n","  prop_amu=int(0.333*prop)\n","  prop_med=int(0.333*prop)\n","  features=[]\n","  label=[]\n","  dic_f={}\n","  tempf,templ=proportion(dic,indice_neutr,prop_neutr)\n","  features+=tempf\n","  label+=templ\n","  tempf,templ=proportion(dic,indice_stress,prop_stress)\n","  features+=tempf\n","  label+=templ\n","  tempf,templ=proportion(dic,indice_amu,prop_amu)\n","  features+=tempf\n","  label+=templ\n","  tempf,templ=proportion(dic,indice_med,prop_med)\n","  features+=tempf\n","  label+=templ\n","  dic_f[\"features\"]=features\n","  dic_f[\"label\"]=label\n","  return dic_f"]},{"cell_type":"markdown","metadata":{"id":"88CdoUVQrpaA"},"source":["# DS creation kfold"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1662666299609,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"UNrIGSslSwbF"},"outputs":[],"source":["class ds_wesad(Dataset):\n","    \"\"\" Define the Dataset object for the WESAD (feature vector;is_stressed(0/1);emotionnal_state(0;1;2;3)\n","    \n","    0 neutral\n","    1 stress\n","    2 amusement\n","    3 relax\n","    \n","    \"\"\"\n","    def __init__(self, dic):\n","        self.samples = []\n","        self.dic=dic\n","        for i in range(0,len(dic[\"label\"])):\n","            num=dic[\"label\"][i]\n","            stress=num==2\n","            x=np.array(dic[\"features\"][i])\n","            self.samples.append((x,int(stress),num))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, id):\n","        return self.samples[id]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33752,"status":"ok","timestamp":1662666333351,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"pXi9BUzeMZ0I","outputId":"d1926b0f-6e16-48e7-983b-094407bd2ec4"},"outputs":[],"source":["\"\"\"Handles the cross validation process, generates the multiple training and validation data dictionnaries for the cross validation\"\"\"\n","\n","name_list= ['WESADECG_S2.json', 'WESADECG_S3.json', 'WESADECG_S4.json', 'WESADECG_S5.json', 'WESADECG_S6.json', 'WESADECG_S7.json',\n"," 'WESADECG_S8.json', 'WESADECG_S9.json', 'WESADECG_S10.json', 'WESADECG_S11.json', 'WESADECG_S13.json', 'WESADECG_S14.json',\n"," 'WESADECG_S15.json', 'WESADECG_S16.json']\n","\n","\n","#assert (len(name_list)==14)  ONE Subject for testing S17\n","list_dic_ds=[]\n","cntr=0\n","\n","for k in range(0,14):\n","  for j in range(k,14):\n","\n","    if k!=j :  \n","      f1 = open(DIR_DATA+name_list[k])\n","      f2 = open(DIR_DATA+name_list[j])\n","      data_1 = json.load(f1)\n","      data_2 = json.load(f2)\n","      dic_3=fusion_dic([data_1,data_2])\n","      dic_v=extract_ds_from_dict(dic_3)  #Create a validation dictionnary made from the data of 2 subject (some data of the subject are deleted because they are irrelevant due to misfunction of sensors/impossible values)\n","      L=[]\n","\n","      for i in range(0,len(name_list)):\n","        if (i!=k and i!=j):\n","          f=open(DIR_DATA+name_list[i])\n","          data = json.load(f)\n","          dic=eq_dic(data)\n","          L.append(dic)\n","\n","      assert (len(L)==12)\n","      dic_4=fusion_dic(L)\n","      dic_t=extract_ds_from_dict(dic_4)\n","      list_dic_ds.append([dic_t,dic_v,k,j]) #Create a training dictionnary made from the data of 12 subjects, all of the data are balanced and extract with impossible (physically) values are deleted\n","      cntr+=1\n","      if cntr%10==0:\n","        print(cntr)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2967,"status":"ok","timestamp":1662666336314,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"OlKq-kz7ULOY"},"outputs":[],"source":["\"\"\" Generate the multiple datasets object for each data dictionnary (first on is training dataset, second is validation dataset)\"\"\"\n","list_ds=[]\n","cntr=0\n","for sample in list_dic_ds:\n","  list_ds.append([ds_wesad(sample[0]),ds_wesad(sample[1]),sample[2],sample[3]]) #[training dataset; validation dataset(from subjects k and j); k; j]\n","  cntr+=1"]},{"cell_type":"markdown","metadata":{"id":"VZVLgHmjVhGa"},"source":["# Model"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1662666336315,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"PN7Ya-GyVgDe"},"outputs":[],"source":["from torch.nn.modules.activation import LeakyReLU\n","\"\"\" DNN Model \"\"\"\n","\n","def init_weight(m):\n","    \"\"\"Initialization of the weights\"\"\"\n","    if isinstance(m,nn.Linear):\n","        nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","    if isinstance(m, nn.BatchNorm1d):\n","        m.weight.data.fill_(1)\n","        m.bias.data.zero_() \n","\n","class ClassifierECG(nn.Module):\n","    \"\"\"DNN model, see the model architecture in the report for more details\"\"\"\n","    def __init__(self, ngpu):\n","        super(ClassifierECG, self).__init__()\n","        self.ngpu = ngpu\n","        self.nnECG = nn.Sequential(\n","            nn.Linear(12,128,bias=True),\n","            nn.BatchNorm1d(128),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(128,64,bias=True),\n","            nn.BatchNorm1d(64),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(64,16,bias=True),\n","            nn.BatchNorm1d(16),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(16,4,bias=True),\n","            nn.BatchNorm1d(4),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(4,1,bias=True),\n","            nn.Sigmoid()\n","        )\n","        self.nnECG.apply(init_weight)\n","\n","    def forward(self, input):\n","        return self.nnECG(input)"]},{"cell_type":"markdown","metadata":{"id":"B6BCMV8WQmJt"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvpAkUbPWJNi"},"outputs":[],"source":["def training(net,dataloader_t,dataloader_v,num_epochs,j,k):\n","  \"\"\" Training with a BCELoss on the dataset, for each epoch the net weights are saved and the mean error is computed to plot the loss\n","  for training and valdiation dataset\n","  \"\"\"\n","  Loss = []\n","  Lossv= []\n","  for epoch in range(num_epochs):\n","      L_t=[]\n","      L_v=[]\n","      for i, dataj in enumerate(dataloader_t, 0):\n","          net.zero_grad()\n","          x=dataj[0].float().to(device)\n","          yhat=dataj[1].float().to(device)\n","          yhat=yhat.view(-1,1)\n","          y=net(x)\n","          err_t=nn.BCELoss()(y.float(),yhat.float())\n","          err_t.backward()\n","          optimizer.step()\n","          L_t.append(err_t.item())\n","      for i, dataj in enumerate(dataloader_v, 0):\n","        net.eval()     \n","        x=dataj[0].float().to(device)\n","        yhat=dataj[1].float().to(device)\n","        yhat=yhat.view(-1,1)\n","        y=net(x)\n","        err_v=nn.BCELoss()(y.float(),yhat.float())\n","        L_v.append(err_v.item())\n","      err=np.mean(L_t)\n","      errv=np.mean(L_v)\n","      Loss.append(err)\n","      Lossv.append(errv)\n","      torch.save(net.state_dict(), DIR_NET_SAVING+\"net_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(epoch)+\".pth\")\n","  return [Lossv,np.argmin(Lossv)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3905637,"status":"ok","timestamp":1662559052495,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"tKLXZEHUZ5Jd","outputId":"a0563ced-18fc-46ed-8925-34218d8ceb81"},"outputs":[],"source":["def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","num_workers = 2\n","batch_size = 32\n","ngpu = 1\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")  \n","\n","L=[]    \n","cntr=0\n","\n","for ds in tqdm(list_ds):\n","  net= ClassifierECG(ngpu).to(device)\n","  lr=0.0001\n","  beta1=0.9\n","  optimizer = optim.Adam(net.parameters(), lr=lr, betas=(beta1, 0.999))\n","  dataset_t=ds[0]\n","  dataset_v=ds[1]\n","  k=ds[2]\n","  j=ds[3]\n","  dataloader_t = torch.utils.data.DataLoader(dataset_t,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  dataloader_v = torch.utils.data.DataLoader(dataset_v,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  L.append(training(net,dataloader_t,dataloader_v,15,j,k))"]},{"cell_type":"markdown","metadata":{"id":"L_av_cuRpqJJ"},"source":["# Save Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxPY4KRsV5TY"},"outputs":[],"source":["class NpEncoder(json.JSONEncoder):\n","    def default(self, obj):\n","        if isinstance(obj, np.integer):\n","            return int(obj)\n","        if isinstance(obj, np.floating):\n","            return float(obj)\n","        if isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        return super(NpEncoder, self).default(obj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgDqQSg8GoBW"},"outputs":[],"source":["with open(DIR_RESULTS+\"results.json\",\"w\") as file:\n","    json.dump(L,file,cls=NpEncoder)"]},{"cell_type":"markdown","metadata":{"id":"WWrbjN7MQiIi"},"source":["# Load Results"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":803,"status":"ok","timestamp":1662666865451,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"MIxyw5dLWyrg"},"outputs":[],"source":["with open(DIR_RESULTS+\"results.json\",\"r\") as file:\n","    L=json.load(file)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":803,"status":"ok","timestamp":1662667016415,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"f51cKjNarS8r"},"outputs":[],"source":["def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","num_workers = 2\n","batch_size = 32\n","ngpu = 1\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":752},"executionInfo":{"elapsed":61379,"status":"ok","timestamp":1662667610491,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"syMnHy0KQiIx","outputId":"1d5a4b80-4a67-44d1-8d69-463ee7870120"},"outputs":[],"source":["\"\"\"\n","This cell compute the confusion matrixes (one with no label and one with label) with the best model of each fold (91 folds)\n","=> (prediction of the network as line label and true emotional state as column label)\n","The ploted matrixes are the mean+/-std of all the confusion matrixes (all folds)\n","\n","This cell also compute the accuracy, precision, recall and F1 score of each fold model and stores these metrics in a list\n","\"\"\"\n","\n","confusionlabelmean=np.zeros((2,4))\n","confusionmean=np.zeros((2,2))\n","acc_list=[]\n","prec_list=[]\n","recall_list=[]\n","f1_list=[]\n","confusion_label_list=[]\n","confusion_list=[]\n","\n","for n in range(0,len(L)):\n","  if n%10==0:\n","    print(n)\n","  k=list_ds[n][2]\n","  j=list_ds[n][3]\n","  dataset_t=list_ds[n][0]\n","  dataset_v=list_ds[n][1]\n","  epch=np.argmin(L[n][0])\n","  net= ClassifierECG(ngpu).to(device)\n","  lr=0.0001\n","  beta1=0.9\n","  optimizer = optim.Adam(net.parameters(), lr=lr, betas=(beta1, 0.999))\n","  net.load_state_dict(torch.load(DIR_NET_SAVING+\"net_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(epch)+\".pth\"))\n","  dataloader_t = torch.utils.data.DataLoader(dataset_t,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  dataloader_v = torch.utils.data.DataLoader(dataset_v,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  trsh=0.5\n","  net.eval()\n","  confusionlabel=np.zeros((2,4))\n","  confusion=np.zeros((2,2))\n","  length_dsv=0\n","  for i, datal in enumerate(dataloader_v, 0):\n","        confusionlabelt,confusiont=conf_mat(net,datal,trsh)\n","        confusion+=confusiont\n","        confusionlabel+=confusionlabelt \n","        length_dsv+=batch_size\n","\n","  TP=confusion[0,0]\n","  TN=confusion[1,1]\n","  FN=confusion[1,0]\n","  FP=confusion[0,1]\n","  acc=(TP+TN)/(TP+FP+FN+TN)\n","  precision=TP/(TP+FP)\n","  recall=TP/(TP+FN)\n","  F1score=(2*recall*precision)/(recall+precision)\n","  acc_list.append(acc)\n","  prec_list.append(precision)\n","  recall_list.append(recall)\n","  f1_list.append(F1score)\n","  confusion_label_list.append(100*confusionlabel/length_dsv)\n","  confusion_list.append(100*confusion/length_dsv)\n","\n","\n","confusionmean=np.round(np.mean(confusion_list,axis=0),3)\n","confusionlabelmean=np.round(np.mean(confusion_label_list,axis=0),3)\n","confusionstd=np.round(np.std(confusion_list,axis=0),3)\n","confusionlabelstd=np.round(np.std(confusion_label_list,axis=0),3)\n","annot_confusion=np.array([str(a)+\"+/-\"+str(b) for a,b in zip(confusionmean.reshape(-1).tolist(),confusionstd.reshape(-1).tolist())]).reshape(confusionmean.shape)\n","annot_confusion_label=np.array([str(a)+\"+/-\"+str(b) for a,b in zip(confusionlabelmean.reshape(-1).tolist(),confusionlabelstd.reshape(-1).tolist())]).reshape(confusionlabelmean.shape)\n","x_axis_confl = ['neutral','stress','amusement','meditation'] # labels for x-axis\n","y_axis_confl = ['stress','no stress'] # labels for y-axis\n","x_axis_conf = ['stress','no stress']\n","y_axis_conf = ['stress','no stress']\n","sns.set(rc={\"figure.figsize\":(15, 5)})\n","fig, axs = plt.subplots(ncols=2,figsize=(33,9))\n","sns.heatmap(confusionmean.astype('int32'), xticklabels=x_axis_conf, yticklabels=y_axis_conf,annot=annot_confusion,ax=axs[0],fmt = '')\n","axs[0].set_xlabel('Ground Truth')\n","axs[0].set_ylabel('Prediction')\n","axs[0].title.set_text('Confusion Matrix label : Stress/No Stress (population in %)')\n","sns.heatmap(confusionlabelmean.astype('int32'), xticklabels=x_axis_confl, yticklabels=y_axis_confl, annot=annot_confusion_label,ax=axs[1],fmt = '')\n","axs[1].set_xlabel('Ground Truth')\n","axs[1].set_ylabel('Prediction')\n","axs[1].title.set_text('Confusion Matrix label : emotionnal state (population in %)')"]},{"cell_type":"markdown","metadata":{"id":"rpW2bsU0o273"},"source":["# Metrics :"]},{"cell_type":"markdown","metadata":{"id":"Q9a8yRKso5F_"},"source":["The 4 following figures are the distribution of accuracy, precision, recall and f1 score of all best models (for each fold)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1662667092261,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"jqNCjrMme7x6","outputId":"d6dd82fb-806b-468a-c0f2-df409549203f"},"outputs":[],"source":["mes=acc_list\n","plt.hist(mes,bins=20)\n","plt.title('Accuracy of all best models')\n","plt.xlabel('Accuracy') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.75, 12, txtm)\n","plt.text(0.75, 11, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":860,"status":"ok","timestamp":1662667093555,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"_bg7zcfmbKcc","outputId":"d173a2c3-6c3b-438b-98be-27945463bf56"},"outputs":[],"source":["mes=prec_list\n","plt.hist(mes,bins=20)\n","plt.title('Precision of all best models')\n","plt.xlabel('Precision') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.35, 13, txtm)\n","plt.text(0.35, 12, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":703,"status":"ok","timestamp":1662667094243,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"cBEjjMt_fcJ-","outputId":"ca444bd6-6c6a-48bd-cfbf-c6d15add9202"},"outputs":[],"source":["mes=recall_list\n","plt.hist(mes,bins=20)\n","plt.title('Recall of all best models')\n","plt.xlabel('Recall') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.2, 43, txtm)\n","plt.text(0.2, 40, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1662667094244,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"7YQ9THArfcue","outputId":"deac025c-1d26-4524-8895-509e1a22cbc6"},"outputs":[],"source":["mes=f1_list\n","plt.hist(mes,bins=20)\n","plt.title('F1 score of all best models')\n","plt.xlabel('F1 score') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.3, 17.5, txtm)\n","plt.text(0.3, 16, txtstd)\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["o4lE6XdnkRCt","88CdoUVQrpaA","VZVLgHmjVhGa","L_av_cuRpqJJ"],"machine_shape":"hm","provenance":[{"file_id":"16DbLIaJKMKs6INHP1tJ_d7p_rPw2OXjP","timestamp":1645823537065},{"file_id":"1qTsAzF2xab3lOSky0DXfIoUUiqD1UHWF","timestamp":1645763189709}]},"kernelspec":{"display_name":"Python 3.9.7 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.7"},"vscode":{"interpreter":{"hash":"acb408c112aa627a318ac6bee697c54a21dc0d988d17c05deacc60f98e48531a"}}},"nbformat":4,"nbformat_minor":0}
