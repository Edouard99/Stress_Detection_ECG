{"cells":[{"cell_type":"markdown","metadata":{"id":"USaorKo7DP9B"},"source":["# Base"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31244,"status":"ok","timestamp":1662666285989,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"gr2Cj9rxQ_NT","outputId":"090b8a4e-5f7d-4062-d4f8-d12597775907"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import matplotlib.animation as animation\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import json\n","import seaborn as sns\n","from tqdm import tqdm\n","from torchinfo import summary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DIR_DATA=\"/content/data/Dataset/\" #Path where your dataset files are stored, this folder contains WESADECG_S2.json,WESADECG_S3.json,...WESADECG_Sk.json files\n","DIR_NET_SAVING=\"/content/data/net/\" #Path where model's weights are saved for each training\n","DIR_RESULTS=\"/content/data/\" #Path to save Results of the network (loss function)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1662666299608,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"9Qy3Go4cE7EZ","outputId":"24581a7b-964c-477a-b32f-aa14cf369112"},"outputs":[],"source":["manualSeed=1\n","torch.manual_seed(manualSeed)\n","random.seed(manualSeed)\n","np.random.seed(manualSeed)\n","g = torch.Generator()\n","g.manual_seed(manualSeed)"]},{"cell_type":"markdown","metadata":{"id":"o4lE6XdnkRCt"},"source":["# Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1662666299609,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"emSxBk7ikS5m"},"outputs":[],"source":["def suppr(dic):\n","  \"\"\" Delete extremums (irrelevant extract due to weird peak detections) from a dictionnary (a subject)  \n","  \"\"\"\n","  bornemax=np.quantile(dic[\"features\"],0.99,axis=0)\n","  bornemin=np.quantile(dic[\"features\"],0.01,axis=0)\n","  indicesmauvais=np.where(np.sum(np.add(bornemin>np.array(dic[\"features\"]),np.array(dic[\"features\"])>bornemax),axis=1)>0)[0]\n","  k=0\n","\n","  for i in indicesmauvais:\n","    del dic[\"features\"][i-k]\n","    del dic[\"label\"][i-k]\n","    k+=1\n","\n","  return dic\n","\n","def extract_ds_from_dict(data):\n","  \"\"\" Delete the irrelevant extract for each state and return the dataset as a dictionnary\"\"\"\n","  Letat=[]\n","\n","  for i in range(0,4):\n","    dictio={}\n","    features=[data[\"features\"][j] for j in np.where(np.array(data[\"label\"])==i+1)[0]] \n","    label=[data[\"label\"][j] for j in np.where(np.array(data[\"label\"])==i+1)[0]]\n","    dictio[\"features\"]=features\n","    dictio[\"label\"]=label\n","    Letat.append(dictio.copy())\n","\n","  neutr=Letat[0]\n","  stress=Letat[1]\n","  amu=Letat[2]\n","  med=Letat[3]\n","  neutr=suppr(neutr)\n","  stress=suppr(stress)\n","  amu=suppr(amu)\n","  med=suppr(med)\n","  features=[]\n","  label=[]\n","  dict_id={}\n","\n","  for m in range(0,4):\n","    dictio=Letat[m]\n","    features+=[x for x in dictio[\"features\"]] \n","    label+=[x for x in dictio[\"label\"]]\n","\n","  dict_id[\"features\"]=features\n","  dict_id[\"label\"]=label\n","  return dict_id.copy()\n","\n","def conf_mat(net,datal,trsh):\n","  \"\"\" Compute 2 confusion matrixes according to a dataloader and a threshold for state prediction (above the threshold the subject is \n","  under stress condition) \n","  \n","  2x2 confusion matrix : Stress and No stress as label for line and column\n","  3x4 confusion matrix : {Total ; Stress ; No stress} as label for the lines\n","                         {Neutral ; Stress ; Amusement; Meditation} as label for the columns\n","  \"\"\"\n","  x=datal[0].float().to(device)\n","  y=net(x).view(-1)\n","  pred=(y>trsh).int()\n","  label=datal[1].float().to(device).view(-1).int()\n","  num=datal[2].float().to(device).int()\n","  comp=torch.eq(label,pred).int()\n","  mat_label=np.zeros((2,4))\n","  mat_nolbl=np.zeros((2,2))\n","\n","  for i in range(0,4):\n","    tens=torch.where(num==i+1,1,0)\n","    numtot=torch.sum(tens).item()\n","    num_G=torch.sum(torch.where(torch.mul(tens,comp)==1,1,0)).item()\n","\n","    if i ==1:\n","      mat_nolbl[0,0]+=num_G\n","      mat_nolbl[1,0]+=numtot-num_G\n","      mat_label[0,i]=num_G\n","      mat_label[1,i]=numtot-num_G\n","\n","    else:\n","      mat_nolbl[1,1]+=num_G\n","      mat_nolbl[0,1]+=numtot-num_G\n","      mat_label[1,i]=num_G\n","      mat_label[0,i]=numtot-num_G\n","\n","\n","  return mat_label,mat_nolbl\n","\n","def fusion_dic(list_dic):\n","  \"\"\" Merge dictionnaries (dataset) from a list (each dictionnary represent the dataset of a subject \"\"\"\n","  features=[]\n","  label=[]\n","  dic_f={}\n","\n","  for dic in list_dic:\n","    features+=dic[\"features\"]\n","    label+=dic[\"label\"]\n","\n","  dic_f[\"features\"]=features\n","  dic_f[\"label\"]=label\n","  return dic_f\n","\n","def proportion(dic, indice, prop):\n","  \"\"\" Return a balanced dataset (reduced by the factor prop) for a balanced training\"\"\"\n","  tot=len(indice)\n","  features=[dic[\"features\"][j] for j in indice[::int(np.ceil(tot/prop))]]\n","  label=[dic[\"label\"][j] for j in indice[::int(np.ceil(tot/prop))]]\n","  return features,label\n","\n","def eq_dic(dic):\n","  \"\"\" Return a balanced dataset from the dictionnary of a subject (same ammount of data for neutral/stress/amusement/relax condition)\"\"\"\n","  indice_neutr=np.where(np.array(dic[\"label\"])==1)[0]\n","  indice_stress=np.where(np.array(dic[\"label\"])==2)[0]\n","  indice_amu=np.where(np.array(dic[\"label\"])==3)[0]\n","  indice_med=np.where(np.array(dic[\"label\"])==4)[0]\n","  nbr_neutr=len(indice_neutr)\n","  nbr_stress=len(indice_stress)\n","  nbr_amu=len(indice_amu)\n","  nbr_med=len(indice_med)\n","  prop=min([3*nbr_neutr,nbr_stress,3*nbr_amu,3*nbr_med])\n","  prop_stress=prop\n","  prop_neutr=int(0.333*prop)\n","  prop_amu=int(0.333*prop)\n","  prop_med=int(0.333*prop)\n","  features=[]\n","  label=[]\n","  dic_f={}\n","  tempf,templ=proportion(dic,indice_neutr,prop_neutr)\n","  features+=tempf\n","  label+=templ\n","  tempf,templ=proportion(dic,indice_stress,prop_stress)\n","  features+=tempf\n","  label+=templ\n","  tempf,templ=proportion(dic,indice_amu,prop_amu)\n","  features+=tempf\n","  label+=templ\n","  tempf,templ=proportion(dic,indice_med,prop_med)\n","  features+=tempf\n","  label+=templ\n","  dic_f[\"features\"]=features\n","  dic_f[\"label\"]=label\n","  return dic_f"]},{"cell_type":"markdown","metadata":{"id":"88CdoUVQrpaA"},"source":["# DS creation kfold"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1662666299609,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"UNrIGSslSwbF"},"outputs":[],"source":["class ds_wesad(Dataset):\n","    \"\"\" Define the Dataset object for the WESAD (feature vector;is_stressed(0/1);emotionnal_state(0;1;2;3)\n","    \n","    0 neutral\n","    1 stress\n","    2 amusement\n","    3 relax\n","    \n","    \"\"\"\n","    def __init__(self, dic):\n","        self.samples = []\n","        self.dic=dic\n","        for i in range(0,len(dic[\"label\"])):\n","            num=dic[\"label\"][i]\n","            stress=num==2\n","            x=np.array(dic[\"features\"][i])\n","            self.samples.append((x,int(stress),num))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, id):\n","        return self.samples[id]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33752,"status":"ok","timestamp":1662666333351,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"pXi9BUzeMZ0I","outputId":"d1926b0f-6e16-48e7-983b-094407bd2ec4"},"outputs":[],"source":["\"\"\"Handles the cross validation process, generates the multiple training and validation data dictionnaries for the cross validation\"\"\"\n","\n","name_list= ['WESADECG_S2.json', 'WESADECG_S3.json', 'WESADECG_S4.json', 'WESADECG_S5.json', 'WESADECG_S6.json', 'WESADECG_S7.json',\n"," 'WESADECG_S8.json', 'WESADECG_S9.json', 'WESADECG_S10.json', 'WESADECG_S11.json', 'WESADECG_S13.json', 'WESADECG_S14.json',\n"," 'WESADECG_S15.json', 'WESADECG_S16.json']\n","\n","\n","#assert (len(name_list)==14)  ONE Subject for testing S17\n","list_dic_ds=[]\n","cntr=0\n","\n","for k in range(0,14):\n","  for j in range(k,14):\n","\n","    if k!=j :  \n","      f1 = open(DIR_DATA+name_list[k])\n","      f2 = open(DIR_DATA+name_list[j])\n","      data_1 = json.load(f1)\n","      data_2 = json.load(f2)\n","      dic_3=fusion_dic([data_1,data_2])\n","      dic_v=extract_ds_from_dict(dic_3)  #Create a validation dictionnary made from the data of 2 subject (some data of the subject are deleted because they are irrelevant due to misfunction of sensors/impossible values)\n","      L=[]\n","\n","      for i in range(0,len(name_list)):\n","        if (i!=k and i!=j):\n","          f=open(DIR_DATA+name_list[i])\n","          data = json.load(f)\n","          dic=eq_dic(data)\n","          L.append(dic)\n","\n","      assert (len(L)==12)\n","      dic_4=fusion_dic(L)\n","      dic_t=extract_ds_from_dict(dic_4)\n","      list_dic_ds.append([dic_t,dic_v,k,j]) #Create a training dictionnary made from the data of 12 subjects, all of the data are balanced and extract with impossible (physically) values are deleted\n","      cntr+=1\n","      if cntr%10==0:\n","        print(cntr)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2967,"status":"ok","timestamp":1662666336314,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"OlKq-kz7ULOY"},"outputs":[],"source":["\"\"\" Generate the multiple datasets object for each data dictionnary (first on is training dataset, second is validation dataset)\"\"\"\n","list_ds=[]\n","cntr=0\n","for sample in list_dic_ds:\n","  list_ds.append([ds_wesad(sample[0]),ds_wesad(sample[1]),sample[2],sample[3]]) #[training dataset; validation dataset(from subjects k and j); k; j]\n","  cntr+=1"]},{"cell_type":"markdown","metadata":{"id":"VZVLgHmjVhGa"},"source":["# Model"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1662666336315,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"PN7Ya-GyVgDe"},"outputs":[],"source":["from torch.nn.modules.activation import LeakyReLU\n","\"\"\" DNN Model \"\"\"\n","\n","def init_weight(m):\n","    \"\"\"Initialization of the weights\"\"\"\n","    if isinstance(m,nn.Linear):\n","        nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","    if isinstance(m, nn.BatchNorm1d):\n","        m.weight.data.fill_(1)\n","        m.bias.data.zero_() \n","\n","class ClassifierECG(nn.Module):\n","    \"\"\"DNN model, see the model architecture in the report for more details\"\"\"\n","    def __init__(self, ngpu):\n","        super(ClassifierECG, self).__init__()\n","        self.ngpu = ngpu\n","        self.nnECG = nn.Sequential(\n","            nn.Linear(12,128,bias=True),\n","            nn.BatchNorm1d(128),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(128,64,bias=True),\n","            nn.BatchNorm1d(64),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(64,16,bias=True),\n","            nn.BatchNorm1d(16),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(16,4,bias=True),\n","            nn.BatchNorm1d(4),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(4,1,bias=True),\n","            nn.Sigmoid()\n","        )\n","        self.nnECG.apply(init_weight)\n","\n","    def forward(self, input):\n","        return self.nnECG(input)"]},{"cell_type":"markdown","metadata":{"id":"B6BCMV8WQmJt"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvpAkUbPWJNi"},"outputs":[],"source":["def training(net,dataloader_t,dataloader_v,num_epochs,j,k):\n","  \"\"\" Training with a BCELoss on the dataset, for each epoch the net weights are saved and the mean error is computed to plot the loss\n","  for training and valdiation dataset\n","  \"\"\"\n","  Loss = []\n","  Lossv= []\n","  for epoch in range(num_epochs):\n","      L_t=[]\n","      L_v=[]\n","      for i, dataj in enumerate(dataloader_t, 0):\n","          net.zero_grad()\n","          x=dataj[0].float().to(device)\n","          yhat=dataj[1].float().to(device)\n","          yhat=yhat.view(-1,1)\n","          y=net(x)\n","          err_t=nn.BCELoss()(y.float(),yhat.float())\n","          err_t.backward()\n","          optimizer.step()\n","          L_t.append(err_t.item())\n","      for i, dataj in enumerate(dataloader_v, 0):\n","        net.eval()     \n","        x=dataj[0].float().to(device)\n","        yhat=dataj[1].float().to(device)\n","        yhat=yhat.view(-1,1)\n","        y=net(x)\n","        err_v=nn.BCELoss()(y.float(),yhat.float())\n","        L_v.append(err_v.item())\n","      err=np.mean(L_t)\n","      errv=np.mean(L_v)\n","      Loss.append(err)\n","      Lossv.append(errv)\n","      torch.save(net.state_dict(), DIR_NET_SAVING+\"net_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(epoch)+\".pth\")\n","  return [Lossv,np.argmin(Lossv)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3905637,"status":"ok","timestamp":1662559052495,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"tKLXZEHUZ5Jd","outputId":"a0563ced-18fc-46ed-8925-34218d8ceb81"},"outputs":[],"source":["def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","num_workers = 2\n","batch_size = 32\n","ngpu = 1\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")  \n","\n","L=[]    \n","cntr=0\n","\n","for ds in tqdm(list_ds):\n","  net= ClassifierECG(ngpu).to(device)\n","  lr=0.0001\n","  beta1=0.9\n","  optimizer = optim.Adam(net.parameters(), lr=lr, betas=(beta1, 0.999))\n","  dataset_t=ds[0]\n","  dataset_v=ds[1]\n","  k=ds[2]\n","  j=ds[3]\n","  dataloader_t = torch.utils.data.DataLoader(dataset_t,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  dataloader_v = torch.utils.data.DataLoader(dataset_v,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  L.append(training(net,dataloader_t,dataloader_v,15,j,k))"]},{"cell_type":"markdown","metadata":{"id":"L_av_cuRpqJJ"},"source":["# Save Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxPY4KRsV5TY"},"outputs":[],"source":["class NpEncoder(json.JSONEncoder):\n","    def default(self, obj):\n","        if isinstance(obj, np.integer):\n","            return int(obj)\n","        if isinstance(obj, np.floating):\n","            return float(obj)\n","        if isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        return super(NpEncoder, self).default(obj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgDqQSg8GoBW"},"outputs":[],"source":["with open(DIR_RESULTS+\"results.json\",\"w\") as file:\n","    json.dump(L,file,cls=NpEncoder)"]},{"cell_type":"markdown","metadata":{"id":"WWrbjN7MQiIi"},"source":["# Load Results"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":803,"status":"ok","timestamp":1662666865451,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"MIxyw5dLWyrg"},"outputs":[],"source":["with open(DIR_RESULTS+\"results.json\",\"r\") as file:\n","    L=json.load(file)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":803,"status":"ok","timestamp":1662667016415,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"f51cKjNarS8r"},"outputs":[],"source":["def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","num_workers = 2\n","batch_size = 32\n","ngpu = 1\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":752},"executionInfo":{"elapsed":61379,"status":"ok","timestamp":1662667610491,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"syMnHy0KQiIx","outputId":"1d5a4b80-4a67-44d1-8d69-463ee7870120"},"outputs":[],"source":["\"\"\"\n","This cell compute the confusion matrixes (one with no label and one with label) with the best model of each fold (91 folds)\n","=> (prediction of the network as line label and true emotional state as column label)\n","The ploted matrixes are the mean+/-std of all the confusion matrixes (all folds)\n","\n","This cell also compute the accuracy, precision, recall and F1 score of each fold model and stores these metrics in a list\n","\"\"\"\n","\n","confusionlabelmean=np.zeros((2,4))\n","confusionmean=np.zeros((2,2))\n","acc_list=[]\n","prec_list=[]\n","recall_list=[]\n","f1_list=[]\n","confusion_label_list=[]\n","confusion_list=[]\n","\n","for n in range(0,len(L)):\n","  if n%10==0:\n","    print(n)\n","  k=list_ds[n][2]\n","  j=list_ds[n][3]\n","  dataset_t=list_ds[n][0]\n","  dataset_v=list_ds[n][1]\n","  epch=np.argmin(L[n][0])\n","  net= ClassifierECG(ngpu).to(device)\n","  lr=0.0001\n","  beta1=0.9\n","  optimizer = optim.Adam(net.parameters(), lr=lr, betas=(beta1, 0.999))\n","  net.load_state_dict(torch.load(DIR_NET_SAVING+\"net_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(epch)+\".pth\"))\n","  dataloader_t = torch.utils.data.DataLoader(dataset_t,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  dataloader_v = torch.utils.data.DataLoader(dataset_v,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  trsh=0.5\n","  net.eval()\n","  confusionlabel=np.zeros((2,4))\n","  confusion=np.zeros((2,2))\n","  length_dsv=0\n","  for i, datal in enumerate(dataloader_v, 0):\n","        confusionlabelt,confusiont=conf_mat(net,datal,trsh)\n","        confusion+=confusiont\n","        confusionlabel+=confusionlabelt \n","        length_dsv+=batch_size\n","\n","  TP=confusion[0,0]\n","  TN=confusion[1,1]\n","  FN=confusion[1,0]\n","  FP=confusion[0,1]\n","  acc=(TP+TN)/(TP+FP+FN+TN)\n","  precision=TP/(TP+FP)\n","  recall=TP/(TP+FN)\n","  F1score=(2*recall*precision)/(recall+precision)\n","  acc_list.append(acc)\n","  prec_list.append(precision)\n","  recall_list.append(recall)\n","  f1_list.append(F1score)\n","  confusion_label_list.append(100*confusionlabel/length_dsv)\n","  confusion_list.append(100*confusion/length_dsv)\n","\n","\n","confusionmean=np.round(np.mean(confusion_list,axis=0),3)\n","confusionlabelmean=np.round(np.mean(confusion_label_list,axis=0),3)\n","confusionstd=np.round(np.std(confusion_list,axis=0),3)\n","confusionlabelstd=np.round(np.std(confusion_label_list,axis=0),3)\n","annot_confusion=np.array([str(a)+\"+/-\"+str(b) for a,b in zip(confusionmean.reshape(-1).tolist(),confusionstd.reshape(-1).tolist())]).reshape(confusionmean.shape)\n","annot_confusion_label=np.array([str(a)+\"+/-\"+str(b) for a,b in zip(confusionlabelmean.reshape(-1).tolist(),confusionlabelstd.reshape(-1).tolist())]).reshape(confusionlabelmean.shape)\n","x_axis_confl = ['neutral','stress','amusement','meditation'] # labels for x-axis\n","y_axis_confl = ['stress','no stress'] # labels for y-axis\n","x_axis_conf = ['stress','no stress']\n","y_axis_conf = ['stress','no stress']\n","sns.set(rc={\"figure.figsize\":(15, 5)})\n","fig, axs = plt.subplots(ncols=2,figsize=(33,9))\n","sns.heatmap(confusionmean.astype('int32'), xticklabels=x_axis_conf, yticklabels=y_axis_conf,annot=annot_confusion,ax=axs[0],fmt = '')\n","axs[0].set_xlabel('Ground Truth')\n","axs[0].set_ylabel('Prediction')\n","axs[0].title.set_text('Confusion Matrix label : Stress/No Stress (population in %)')\n","sns.heatmap(confusionlabelmean.astype('int32'), xticklabels=x_axis_confl, yticklabels=y_axis_confl, annot=annot_confusion_label,ax=axs[1],fmt = '')\n","axs[1].set_xlabel('Ground Truth')\n","axs[1].set_ylabel('Prediction')\n","axs[1].title.set_text('Confusion Matrix label : emotionnal state (population in %)')"]},{"cell_type":"markdown","metadata":{"id":"rpW2bsU0o273"},"source":["# Metrics :"]},{"cell_type":"markdown","metadata":{"id":"Q9a8yRKso5F_"},"source":["The 4 following figures are the distribution of accuracy, precision, recall and f1 score of all best models (for each fold)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1662667092261,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"jqNCjrMme7x6","outputId":"d6dd82fb-806b-468a-c0f2-df409549203f"},"outputs":[],"source":["mes=acc_list\n","plt.hist(mes,bins=20)\n","plt.title('Accuracy of all best models')\n","plt.xlabel('Accuracy') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.75, 12, txtm)\n","plt.text(0.75, 11, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":860,"status":"ok","timestamp":1662667093555,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"_bg7zcfmbKcc","outputId":"d173a2c3-6c3b-438b-98be-27945463bf56"},"outputs":[],"source":["mes=prec_list\n","plt.hist(mes,bins=20)\n","plt.title('Precision of all best models')\n","plt.xlabel('Precision') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.35, 13, txtm)\n","plt.text(0.35, 12, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":703,"status":"ok","timestamp":1662667094243,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"cBEjjMt_fcJ-","outputId":"ca444bd6-6c6a-48bd-cfbf-c6d15add9202"},"outputs":[],"source":["mes=recall_list\n","plt.hist(mes,bins=20)\n","plt.title('Recall of all best models')\n","plt.xlabel('Recall') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.2, 43, txtm)\n","plt.text(0.2, 40, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1662667094244,"user":{"displayName":"Edouard .C","userId":"07320798258634684264"},"user_tz":-120},"id":"7YQ9THArfcue","outputId":"deac025c-1d26-4524-8895-509e1a22cbc6"},"outputs":[],"source":["mes=f1_list\n","plt.hist(mes,bins=20)\n","plt.title('F1 score of all best models')\n","plt.xlabel('F1 score') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.3, 17.5, txtm)\n","plt.text(0.3, 16, txtstd)\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["o4lE6XdnkRCt","88CdoUVQrpaA","VZVLgHmjVhGa","L_av_cuRpqJJ"],"machine_shape":"hm","provenance":[{"file_id":"16DbLIaJKMKs6INHP1tJ_d7p_rPw2OXjP","timestamp":1645823537065},{"file_id":"1qTsAzF2xab3lOSky0DXfIoUUiqD1UHWF","timestamp":1645763189709}]},"kernelspec":{"display_name":"Python 3.9.7 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.7"},"vscode":{"interpreter":{"hash":"acb408c112aa627a318ac6bee697c54a21dc0d988d17c05deacc60f98e48531a"}}},"nbformat":4,"nbformat_minor":0}
